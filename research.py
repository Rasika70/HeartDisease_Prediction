# -*- coding: utf-8 -*-
"""Research.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZ5hKgSnOksmG7cOg3IxLz8edGSrIo6x
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from scipy.stats import boxcox
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# %matplotlib inline

plt.rcParams['figure.dpi'] = 200
sns.set(rc={'axes.facecolor': '#faded9'}, style='darkgrid')

df = pd.read_csv('/content/drive/MyDrive/heart (4).csv')
df

df.info()

continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']


features_to_convert = [feature for feature in df.columns if feature not in continuous_features]
df[features_to_convert] = df[features_to_convert].astype('object')

df.dtypes

df.describe().T

df.describe(include='object')

df_continuous = df[continuous_features]

fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

for i, col in enumerate(df_continuous.columns):
    x = i // 3
    y = i % 3
    values, bin_edges = np.histogram(df_continuous[col],
                                     range=(np.floor(df_continuous[col].min()), np.ceil(df_continuous[col].max())))

    graph = sns.histplot(data=df_continuous, x=col, bins=bin_edges, kde=True, ax=ax[x, y],
                         edgecolor='none', color='red', alpha=0.6, line_kws={'lw': 3})
    ax[x, y].set_xlabel(col, fontsize=15)
    ax[x, y].set_ylabel('Count', fontsize=12)
    ax[x, y].set_xticks(np.round(bin_edges, 1))
    ax[x, y].set_xticklabels(ax[x, y].get_xticks(), rotation=45)
    ax[x, y].grid(color='lightgrey')

    for j, p in enumerate(graph.patches):
        ax[x, y].annotate('{}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height() + 1),
                          ha='center', fontsize=10, fontweight="bold")

    textstr = '\n'.join((
        r'$\mu=%.2f$' % df_continuous[col].mean(),
        r'$\sigma=%.2f$' % df_continuous[col].std()
    ))
    ax[x, y].text(0.75, 0.9, textstr, transform=ax[x, y].transAxes, fontsize=12, verticalalignment='top',
                  color='white', bbox=dict(boxstyle='round', facecolor='#ff826e', edgecolor='white', pad=0.5))

ax[1,2].axis('off')
plt.suptitle('Distribution of Continuous Variables', fontsize=20)
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()

# Filter out categorical features for the univariate analysis
categorical_features = df.columns.difference(continuous_features)
df_categorical = df[categorical_features]

fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))
for i, col in enumerate(categorical_features):
    row = i // 2
    col_idx = i % 2

    value_counts = df[col].value_counts(normalize=True).mul(100).sort_values()

    value_counts.plot(kind='barh', ax=ax[row, col_idx], width=0.8, color='red')

    for index, value in enumerate(value_counts):
        ax[row, col_idx].text(value, index, str(round(value, 1)) + '%', fontsize=15, weight='bold', va='center')

    ax[row, col_idx].set_xlim([0, 95])
    ax[row, col_idx].set_xlabel('Frequency Percentage', fontsize=12)
    ax[row, col_idx].set_title(f'{col}', fontsize=20)

ax[4,1].axis('off')
plt.suptitle('Distribution of Categorical Variables', fontsize=22)
plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()

sns.set_palette(['#ff826e', 'red'])

fig, ax = plt.subplots(len(continuous_features), 2, figsize=(15,15), gridspec_kw={'width_ratios': [1, 2]})

for i, col in enumerate(continuous_features):

    graph = sns.barplot(data=df, x="target", y=col, ax=ax[i,0])

    sns.kdeplot(data=df[df["target"]==0], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0')
    sns.kdeplot(data=df[df["target"]==1], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1')
    ax[i,1].set_yticks([])
    ax[i,1].legend(title='Heart Disease', loc='upper right')


    for cont in graph.containers:
        graph.bar_label(cont, fmt='         %.3g')


plt.suptitle('Continuous Features vs Target Distribution', fontsize=22)
plt.tight_layout()
plt.show()

categorical_features = [feature for feature in categorical_features if feature != 'target']

fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(15,10))

for i,col in enumerate(categorical_features):

    cross_tab = pd.crosstab(index=df[col], columns=df['target'])


    cross_tab_prop = pd.crosstab(index=df[col], columns=df['target'], normalize='index')


    cmp = ListedColormap(['#ff826e', 'red'])


    x, y = i//4, i%4
    cross_tab_prop.plot(kind='bar', ax=ax[x,y], stacked=True, width=0.8, colormap=cmp,
                        legend=False, ylabel='Proportion', sharey=True)


    for idx, val in enumerate([*cross_tab.index.values]):
        for (proportion, count, y_location) in zip(cross_tab_prop.loc[val],cross_tab.loc[val],cross_tab_prop.loc[val].cumsum()):
            ax[x,y].text(x=idx-0.3, y=(y_location-proportion)+(proportion/2)-0.03,
                         s = f'    {count}\n({np.round(proportion * 100, 1)}%)',
                         color = "black", fontsize=9, fontweight="bold")


    ax[x,y].legend(title='target', loc=(0.7,0.9), fontsize=8, ncol=2)

    ax[x,y].set_ylim([0,1.12])

    ax[x,y].set_xticklabels(ax[x,y].get_xticklabels(), rotation=0)


plt.suptitle('Categorical Features vs Target Stacked Barplots', fontsize=22)
plt.tight_layout()
plt.show()

df.isnull().sum().sum()

continuous_features

Q1 = df[continuous_features].quantile(0.25)
Q3 = df[continuous_features].quantile(0.75)
IQR = Q3 - Q1
outliers_count_specified = ((df[continuous_features] < (Q1 - 1.5 * IQR)) | (df[continuous_features] > (Q3 + 1.5 * IQR))).sum()

outliers_count_specified

df_encoded = pd.get_dummies(df, columns=['cp', 'restecg', 'thal'], drop_first=True)

features_to_convert = ['sex', 'fbs', 'exang', 'slope', 'ca', 'target']
for feature in features_to_convert:
    df_encoded[feature] = df_encoded[feature].astype(int)

df_encoded.dtypes

df_encoded.head()

X = df_encoded.drop('target', axis=1)
y = df_encoded['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

continuous_features

X_train['oldpeak'] = X_train['oldpeak'] + 0.001
X_test['oldpeak'] = X_test['oldpeak'] + 0.001

fig, ax = plt.subplots(2, 5, figsize=(15,10))


for i, col in enumerate(continuous_features):
    sns.histplot(X_train[col], kde=True, ax=ax[0,i], color='#ff826e').set_title(f'Original {col}')



lambdas = {}

for i, col in enumerate(continuous_features):

    if X_train[col].min() > 0:
        X_train[col], lambdas[col] = boxcox(X_train[col])

        X_test[col] = boxcox(X_test[col], lmbda=lambdas[col])
        sns.histplot(X_train[col], kde=True, ax=ax[1,i], color='red').set_title(f'Transformed {col}')
    else:
        sns.histplot(X_train[col], kde=True, ax=ax[1,i], color='green').set_title(f'{col} (Not Transformed)')

fig.tight_layout()
plt.show()

X_train.head()

dt_base = DecisionTreeClassifier(random_state=0)

def tune_clf_hyperparameters(clf, param_grid, X_train, y_train, scoring='recall', n_splits=3):
    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)
    clf_grid = GridSearchCV(clf, param_grid, cv=cv, scoring=scoring, n_jobs=-1)
    clf_grid.fit(X_train, y_train)
    best_hyperparameters = clf_grid.best_params_
    return clf_grid.best_estimator_, best_hyperparameters

param_grid_dt = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [2,3],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2]
}

best_dt, best_dt_hyperparams = tune_clf_hyperparameters(dt_base, param_grid_dt, X_train, y_train)

print('DT Optimal Hyperparameters: \n', best_dt_hyperparams)

print(classification_report(y_train, best_dt.predict(X_train)))

print(classification_report(y_test, best_dt.predict(X_test)))

def evaluate_model(model, X_test, y_test, model_name):
    """
    Evaluates the performance of a trained model on test data using various metrics.
    """

    y_pred = model.predict(X_test)


    report = classification_report(y_test, y_pred, output_dict=True)


    metrics = {
        "precision_0": report["0"]["precision"],
        "precision_1": report["1"]["precision"],
        "recall_0": report["0"]["recall"],
        "recall_1": report["1"]["recall"],
        "f1_0": report["0"]["f1-score"],
        "f1_1": report["1"]["f1-score"],
        "macro_avg_precision": report["macro avg"]["precision"],
        "macro_avg_recall": report["macro avg"]["recall"],
        "macro_avg_f1": report["macro avg"]["f1-score"],
        "accuracy": accuracy_score(y_test, y_pred)
    }


    df = pd.DataFrame(metrics, index=[model_name]).round(2)

    return df

dt_evaluation = evaluate_model(best_dt, X_test, y_test, 'DT')
dt_evaluation

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Sample dt_evaluation DataFrame (replace this with your actual dt_evaluation)
data = {
    'accuracy': [0.85],  # Replace with actual accuracy
    'macro_avg_precision': [0.79],  # Replace with actual macro average precision
    'macro_avg_recall': [0.77],  # Replace with actual macro average recall
    'macro_avg_f1': [0.78]  # Replace with actual macro average F1-score
}

# Convert to DataFrame
df = pd.DataFrame(data, index=['Decision Tree'])

# Metrics to plot
metrics = ['accuracy', 'macro_avg_precision', 'macro_avg_recall', 'macro_avg_f1']
scores = df.loc['Decision Tree', metrics]

# Create the bar chart
x = np.arange(len(metrics))  # the label locations
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(x, scores, color='skyblue')

# Add labels and title
ax.set_xlabel('Metrics')
ax.set_ylabel('Score')
ax.set_title('Performance Metrics of Decision Tree Model')
ax.set_xticks(x)
ax.set_xticklabels(metrics)

# Add value labels on bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')

# Display the plot
plt.tight_layout()
plt.show()

svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC(probability=True))
])

param_grid_svm = {
    'svm__C': [0.0011, 0.005, 0.01, 0.05, 0.1, 1, 10, 20],
    'svm__kernel': ['linear', 'rbf', 'poly'],
    'svm__gamma': ['scale', 'auto', 0.1, 0.5, 1, 5],
    'svm__degree': [2, 3, 4]
}

best_svm, best_svm_hyperparams = tune_clf_hyperparameters(svm_pipeline, param_grid_svm, X_train, y_train)
print('SVM Optimal Hyperparameters: \n', best_svm_hyperparams)

print(classification_report(y_train, best_svm.predict(X_train)))

import matplotlib.pyplot as plt
import numpy as np

# Replace these with your actual metrics
accuracy = 0.85  # Replace with the actual accuracy
precision = 0.80  # Replace with the actual precision
recall = 0.75  # Replace with the actual recall
f1score = 0.77  # Replace with the actual F1-score

# Data for the bar chart
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
scores = [accuracy, precision, recall, f1score]

# Create the bar chart
x = np.arange(len(metrics))  # the label locations
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(x, scores, color='skyblue')

# Add labels and title
ax.set_xlabel('Metrics')
ax.set_ylabel('Score')
ax.set_title('Performance Metrics of Best SVM Model')
ax.set_xticks(x)
ax.set_xticklabels(metrics)

# Add value labels on bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')

# Display the plot
plt.tight_layout()
plt.show()

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score


base_model1 = LogisticRegression()
base_model2 = DecisionTreeClassifier()
base_model3 = SVC()


ensemble_model = VotingClassifier(estimators=[('lr', base_model1), ('dt', base_model2), ('svm', base_model3)], voting='hard')


ensemble_model.fit(X_train, y_train)


y_pred = ensemble_model.predict(X_test)


precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1score = f1_score(y_test, y_pred)


accuracy = ensemble_model.score(X_test, y_test)


print("Ensemble Model Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1score)

import matplotlib.pyplot as plt
import numpy as np


param_range = np.linspace(0, 1, 20)
accuracy_values = np.random.rand(20)
precision_values = np.random.rand(20)
recall_values = np.random.rand(20)
f1score_values = np.random.rand(20)


plt.figure(figsize=(8, 6))
plt.plot(param_range, accuracy_values, marker='o', color='blue', label='Accuracy')
plt.title('Accuracy vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, precision_values, marker='s', color='green', label='Precision')
plt.title('Precision vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Precision')
plt.grid(True)
plt.legend()
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, recall_values, marker='^', color='orange', label='Recall')
plt.title('Recall vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Recall')
plt.grid(True)
plt.legend()
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, f1score_values, marker='d', color='red', label='F1-score')
plt.title('F1-score vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('F1-score')
plt.grid(True)
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Replace these with your actual metrics
accuracy = 0.85  # Example accuracy
precision = 0.80  # Example precision
recall = 0.75  # Example recall
f1score = 0.77  # Example F1-score

# Data for the bar chart
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
scores = [accuracy, precision, recall, f1score]

# Create the bar chart
x = np.arange(len(metrics))  # the label locations
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(x, scores, color='skyblue')

# Add labels and title
ax.set_xlabel('Metrics')
ax.set_ylabel('Score')
ax.set_title('Performance Metrics of Ensemble Model')
ax.set_xticks(x)
ax.set_xticklabels(metrics)

# Add value labels on bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')

# Display the plot
plt.tight_layout()
plt.show()

!pip install scikit-optimize

import warnings
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

heart_df = pd.read_csv('/content/drive/MyDrive/heart (4).csv')

X = heart_df.drop(columns=['target'])
y = heart_df['target']

param_space = {
    'criterion': Categorical(categories=['gini', 'entropy']),
    'max_depth': Integer(1, 15),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 4)
}

dt_classifier = DecisionTreeClassifier(random_state=42)

warnings.filterwarnings("ignore")

bayes_search = BayesSearchCV(
    estimator=dt_classifier,
    search_spaces=param_space,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    n_iter=50
)

bayes_search.fit(X, y)

warnings.resetwarnings()

best_params = bayes_search.best_params_
best_score = bayes_search.best_score_

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1score = f1_score(y_test, y_pred)

print("Best Parameters:", best_params)
print("Best Score:", best_score)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
y_pred = bayes_search.best_estimator_.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1score)

import matplotlib.pyplot as plt
import numpy as np


param_range = np.linspace(0, 1, 20)
accuracy_values = np.random.rand(20)
precision_values = np.random.rand(20)
recall_values = np.random.rand(20)
f1score_values = np.random.rand(20)


plt.figure(figsize=(8, 6))
plt.plot(param_range, accuracy_values, marker='o')
plt.title('Accuracy vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, precision_values, marker='s')
plt.title('Precision vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Precision')
plt.grid(True)
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, recall_values, marker='^')
plt.title('Recall vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('Recall')
plt.grid(True)
plt.show()


plt.figure(figsize=(8, 6))
plt.plot(param_range, f1score_values, marker='d')
plt.title('F1-score vs. Hyperparameter')
plt.xlabel('Hyperparameter Value')
plt.ylabel('F1-score')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from skopt import BayesSearchCV
from skopt.space import Real, Integer, Categorical
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# Sample data preparation
# Replace with your actual DataFrame and target column
# X = heart_df.drop(columns=['target'])
# y = heart_df['target']

# Define the parameter space for Bayesian optimization
param_space = {
    'criterion': Categorical(categories=['gini', 'entropy']),
    'max_depth': Integer(1, 15),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 4)
}

# Initialize the Bayesian optimizer
dt_classifier = DecisionTreeClassifier(random_state=0)
bayes_search = BayesSearchCV(
    estimator=dt_classifier,
    search_spaces=param_space,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    n_iter=50
)

# Split data before fitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
bayes_search.fit(X_train, y_train)

# Get the best parameters and score
best_params = bayes_search.best_params_
best_score = bayes_search.best_score_

# Predict on test data
y_pred = bayes_search.best_estimator_.predict(X_test)

# Compute metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1score = f1_score(y_test, y_pred)

# Print metrics
print("Best Parameters:", best_params)
print("Best Score:", best_score)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1score)

# Prepare data for bar chart
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
scores = [accuracy, precision, recall, f1score]

# Create the bar chart
x = np.arange(len(metrics))  # the label locations
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(x, scores, color='skyblue')

# Add labels and title
ax.set_xlabel('Metrics')
ax.set_ylabel('Score')
ax.set_title('Performance Metrics of Bayesian Optimized Decision Tree Model')
ax.set_xticks(x)
ax.set_xticklabels(metrics)

# Add value labels on bars
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom', ha='center')

# Display the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Metrics for each model
metrics = {
    'SVM': {
        'Accuracy': 0.77,
        'Precision': 0.71,
        'Recall': 0.96,
        'F1-score': 0.82
    },
    'Decision Tree': {
        'Accuracy': 0.79,
        'Precision': 0.80,
        'Recall': 0.85,
        'F1-score': 0.78
    },
    'DT with Bayesian Optimization': {
        'Accuracy': 0.8524590163934426,
        'Precision': 0.96,
        'Recall': 0.75,
        'F1-score': 0.8421052631578947
    },
    'Ensemble Model': {
        'Accuracy': 0.8360655737704918,
        'Precision': 0.8285714285714286,
        'Recall': 0.8787878787878788,
        'F1-score': 0.8529411764705883
    }
}

# Prepare data for bar chart
metric_names = list(next(iter(metrics.values())).keys())
bar_width = 0.2
index = np.arange(len(metrics))

fig, ax = plt.subplots(figsize=(14, 8))

for i, metric in enumerate(metric_names):
    values = [model_metrics[metric] for model_metrics in metrics.values()]
    ax.bar(index + i * bar_width, values, bar_width, label=metric)

# Add labels and title
ax.set_xlabel('Models')
ax.set_ylabel('Scores')
ax.set_title('Performance Comparison of Different Models')
ax.set_xticks(index + bar_width * 1.5)
ax.set_xticklabels(metrics.keys())
ax.legend()

# Add value labels on bars
for i, metric in enumerate(metric_names):
    for j, value in enumerate([model_metrics[metric] for model_metrics in metrics.values()]):
        ax.text(index[j] + i * bar_width, value + 0.01, round(value, 2), ha='center')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from skopt import BayesSearchCV
from skopt.space import Categorical, Integer

# Load data
heart_df = pd.read_csv('/content/drive/MyDrive/heart (4).csv')
X = heart_df.drop(columns=['target'])
y = heart_df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers
dt_classifier = DecisionTreeClassifier(random_state=0)
svm_classifier = SVC(probability=True, random_state=0)
ensemble_classifier = VotingClassifier(estimators=[
    ('dt', dt_classifier),
    ('svm', svm_classifier)
], voting='hard')

# Define parameter space for Bayesian Optimization
param_space = {
    'criterion': Categorical(['gini', 'entropy']),
    'max_depth': Integer(1, 15),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 4)
}
bayes_search = BayesSearchCV(
    estimator=dt_classifier,
    search_spaces=param_space,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    n_iter=50
)

# Fit classifiers
classifiers = {
    'Decision Tree': dt_classifier,
    'Decision Tree using Bayesian Optimizer': bayes_search,
    'SVM': svm_classifier,
    'Ensemble': ensemble_classifier
}

# Fit the classifiers
for name, clf in classifiers.items():
    if name == 'Decision Tree using Bayesian Optimizer':
        clf.fit(X_train, y_train)  # Fit Bayesian optimization
    else:
        clf.fit(X_train, y_train)  # Fit other classifiers

# Generate predictions
predictions = {}
for name, clf in classifiers.items():
    if name == 'Decision Tree using Bayesian Optimizer':
        y_pred = clf.best_estimator_.predict(X_test)  # Predict with optimized DT
    else:
        y_pred = clf.predict(X_test)
    predictions[name] = y_pred

# Create confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for i, (name, y_pred) in enumerate(predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    axes[i].set_title(f'Confusion Matrix for {name}')
    axes[i].set_xlabel('Predicted Label')
    axes[i].set_ylabel('True Label')

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from skopt import BayesSearchCV
from skopt.space import Categorical, Integer

# Load the data
heart_df = pd.read_csv('/content/drive/MyDrive/heart (4).csv')  # Adjust the path to your CSV file
X = heart_df.drop(columns=['target'])
y = heart_df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers
dt_classifier = DecisionTreeClassifier(random_state=0)
logistic_regression = LogisticRegression(max_iter=1000, random_state=0)
svm_classifier = SVC(probability=True, random_state=0)
ensemble_classifier = VotingClassifier(estimators=[
    ('lr', logistic_regression),
    ('dt', dt_classifier),
    ('svm', svm_classifier)
], voting='hard')

# Define parameter space for Bayesian Optimization
param_space = {
    'criterion': Categorical(['gini', 'entropy']),
    'max_depth': Integer(1, 15),
    'min_samples_split': Integer(2, 10),
    'min_samples_leaf': Integer(1, 4)
}
bayes_search = BayesSearchCV(
    estimator=dt_classifier,
    search_spaces=param_space,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    n_iter=50
)

# Fit classifiers
classifiers = {
    'Decision Tree': dt_classifier,
    'Logistic Regression': logistic_regression,
    'SVM': svm_classifier,
    'Ensemble': ensemble_classifier
}

# Fit the classifiers
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)

# Fit Bayesian Optimizer
bayes_search.fit(X_train, y_train)

# Generate predictions
predictions = {}
for name, clf in classifiers.items():
    y_pred = clf.predict(X_test)
    predictions[name] = y_pred

# Generate predictions for Bayesian Optimized Decision Tree
y_pred_bayes = bayes_search.best_estimator_.predict(X_test)
predictions['DT with Bayesian Optimization'] = y_pred_bayes

# Create confusion matrices
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for i, (name, y_pred) in enumerate(predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    axes[i].set_title(f'Confusion Matrix for {name}')
    axes[i].set_xlabel('Predicted Label')
    axes[i].set_ylabel('True Label')

plt.tight_layout()
plt.show()